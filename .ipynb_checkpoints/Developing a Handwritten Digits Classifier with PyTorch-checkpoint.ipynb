{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b83ca8",
   "metadata": {},
   "source": [
    "# Objectives of the project.\n",
    "\n",
    "In this project, we will cover the following.\n",
    "\n",
    "Let's get stated by importing all the necessary PyTorch and Python Libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ad7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets \n",
    "%matplotlib inline\n",
    "import cv2, matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98bbea",
   "metadata": {},
   "source": [
    "# Introduction to MNIST Dataset\n",
    "\n",
    "MNIST is a popular handwritten numeric dataset containing 60,000 handwritten numbers for machine learning model training and 10,000 handwritten numbers for model testing. This dataset is very useful for testing a model or a new algorithm, if the model has been tested on MNIST and achieves high accuracy, the new algorithm is probably correct. If the algorithm doesn't work on MNIST, it won't work at all.\n",
    "\n",
    "The following code shows how MNIST dataset is been loaded from `torchvision.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101213a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.1307,), std=(0.3081)),\n",
    "                                transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "\n",
    "training_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size= len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13f973",
   "metadata": {},
   "source": [
    "In the above code, we put three transformations together, first, we convert the NumPy array to tensor and then normalize the tensors, the PyTorch implementation of \"normalize\" subtracts from each MNIST image the mean 0.1307, for each channel separately and since we have a grayscale image which is 2-D, we have (0.1307), (0.1307), for the first and second channel respectively and then divide by the standard deviation 0.3081. These 0.1307 and 0.3081 are mean and standard deviation of the training data. This transformation causes each of the input images of our MNIST dataset to be transformed from the pixel value of [0, 255] to [0, 1], this will allow our model to converge in the correct direction and also helps to achieve better accuracy of the model, Next we flatten these images before sending them to our model , this process converts the 28X28 dimensional arrays into a 1-D array (1,28*28), we do this  because  1-D arrays take less amount of memory compared to   multi- Dimension arrays, and thus reduce the time to train the model. Then we specify the root directory called \"data\" where we want to save the MNIST dataset after download, for `training_data` we specify that we want to download only the training data by specifying` train = True`, then we have the transformation applied, for `test_data` we specify train as `False` to specify test data. Then we generate training and test DataLoader which retrieved 16 data points randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa17b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training image and training target shapes are: \n",
      " \tX -torch.Size([60000, 28, 28]) \n",
      "\tY -torch.Size([60000])\n",
      "       \ty-Unique Values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "The task is 10 class classification problem.\n",
      "The testing image and testing target shapes are: \n",
      " \tX -torch.Size([10000, 28, 28]) \n",
      "\tY -torch.Size([10000])\n",
      "       \ty-Unique Values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "tr_image=training_data.data\n",
    "tr_target = training_data.targets\n",
    "tr_classes = tr_target.unique()\n",
    "\n",
    "val_image=test_data.data\n",
    "val_target = test_data.targets\n",
    "val_classes = val_target.unique()\n",
    "print(f'The training image and training target shapes are: \\n \\tX -{tr_image.shape} \\n\\tY -{tr_target.shape}\\n \\\n",
    "      \\ty-Unique Values: {tr_classes}')\n",
    "print(f'The task is {len(tr_classes)} class classification problem.' )\n",
    "\n",
    "print(f'The testing image and testing target shapes are: \\n \\tX -{val_image.shape} \\n\\tY -{val_target.shape}\\n \\\n",
    "      \\ty-Unique Values: {val_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d8a27",
   "metadata": {},
   "source": [
    "From the output above, we can see that the MNIST dataset has 60,000 training images, and each handwritten digit is stored in a grayscale image with size of 28X28 pixels, and the validation data has 10,000 handwritten images of size 28X28 pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9273f5",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decdf51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows, cols = 3,3 #len(tr_classes) , 10\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(5,5))\n",
    "for target_class, row in enumerate(axes):\n",
    "    label_x_rows = np.where(tr_target == target_class)[0]\n",
    "    \n",
    "    for cell in row:\n",
    "        cell.grid(False); cell.axis('off')\n",
    "        ix = np.random.choice(label_x_rows)\n",
    "        x, y = tr_image[ix], tr_target[ix]\n",
    "        cell.imshow(x, cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74660087",
   "metadata": {},
   "source": [
    "From the above output we see that each digit in the MNIST dataset is a grayscale image with a size of 28x28 pixels, all digits in this dataset are size normalized and centered in the image and therefore no other pre-processing technique is required. Next, lets try to explore our transformed data to get a clear picture of the images we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41aba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 784])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325c214",
   "metadata": {},
   "source": [
    "we iterated over the train_loader, In each iteration we get the images and the labels of the images. which we saved as the \"train_features\" which is of the size 32X784, which means  we have 32 images of size 1x784 and the \"train_labels\" which contains 32 labels. The goal of this project is to build a neural network to predict the class of each given input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdced0c0",
   "metadata": {},
   "source": [
    "# Building Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a0b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0838f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2= nn.Linear(500,500)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3=nn.Linear(500,500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(500,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aea9988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=500, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=500, out_features=500, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=500, out_features=500, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc4): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a26a2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#Initialize Adam optimizer and this tell the model what parameters of the model to be updating.\n",
    "optimizer = Adam(params= model.parameters(),lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65095463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training_loss(inputs, labels, model, loss_fn, opt):\n",
    "    \n",
    "    # Put the model in the training model\n",
    "    model.train()\n",
    "    \n",
    "    # Make the prediction for the training inputs\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # compute the error\n",
    "    batch_loss = loss_fn(outputs, labels)\n",
    "    \n",
    "    # \n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # compute the gradients of the batch loss \n",
    "    batch_loss.backward()\n",
    "    \n",
    "    # make the optimizer iterates over all the parameters and use their internally stored gradient to update their values \n",
    "    opt.step()\n",
    "    \n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "567e15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation_loss(inputs, labels, model,loss_fn):\n",
    "    \n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Compute the prediction of the validation set\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Compute the loss values of the prediction on the validation set\n",
    "    val_loss = loss_fn(outputs, labels)\n",
    "    \n",
    "    return val_loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eb92534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(inputs, labels, model):\n",
    "    \n",
    "    # deactivate the autograd to compute accuracy  \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # put the model in evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Make the predictions\n",
    "        output = model(inputs)\n",
    "        \n",
    "    #Returns the maximum value and the argument maxes of all elements in our prediction output\n",
    "    _, argmaxes = output.max(-1)\n",
    "    \n",
    "    # check if the predictions are correct with the labels and store as corect.\n",
    "    correct = argmaxes == labels\n",
    "    \n",
    "    return correct.cpu().numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74382133",
   "metadata": {},
   "source": [
    "\n",
    "Torch.no_grad() deactivates autograd engine. Eventually it will reduce the memory usage and speed up computations.\n",
    "\n",
    "Use of Torch.no_grad():\n",
    "\n",
    "To perform inference without Gradient Calculation.\n",
    "\n",
    "To make sure there's no leak test data into the model.\n",
    "\n",
    "It's generally used to perform Validation. Reason in this case one can use validation batch of large size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa2f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c8627d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs, model, loss_fun, optim, train_dl, test_dl):\n",
    "\n",
    "    # Set the over all  training loss and over all  training accuracy as empty  \n",
    "    training_losses, training_accuracies  =  [], []\n",
    "\n",
    "    # Set the overall validation loss and  validation accuracy as empty \n",
    "    validation_losses, validation_accuracies = [], []\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "    \n",
    "        # Create empty array to save the training losses and training accuracies of each batch as the model train \n",
    "        batch_losses, batch_accuracies = [], []\n",
    "     \n",
    "        #iterate through the batches of the training data loader which is the variable \"data\", note that, \n",
    "        #the 'data' variable contains one batch of the training loader, we compute the accuracies and losses \n",
    "        # enumerate enable us to be able to print status during each epoch expecially when the data is big.    \n",
    "        \n",
    "        for i, data in enumerate(iter(train_loader)):\n",
    "            \n",
    "            # Get the training inputs and labels of one batch\n",
    "            inputs, labels = data\n",
    "        \n",
    "            # Compute the batch loss i.e the loss value with in one batch of the training data\n",
    "            data_loss = batch_training_loss(inputs, labels, model, loss_fn, optimizer)\n",
    "        \n",
    "            # Append the loss of each batch to the batches_losses variable\n",
    "            batch_losses.append(data_loss)\n",
    "        \n",
    "        # After iterating over all batches, computed each batch loss values in the train loader, \n",
    "        #we Compute the average of this batch loses for every epoch(i.e the number times that the \n",
    "        #learning algorithms work through the entire training dataset) and this make epoch_training_loss variable\n",
    "        epoch_training_loss = np.array(batch_losses).mean()\n",
    "    \n",
    "        #\n",
    "        training_losses.append(epoch_training_loss)\n",
    "    \n",
    "    \n",
    "        # Iterate through the batches of the train data loader and compute the  training accuracy\n",
    "        for i, data in enumerate(iter(train_loader)):\n",
    "            \n",
    "            # Get the training inputs and labels of one batch\n",
    "            inputs, labels = data\n",
    "        \n",
    "            # Compute the batch accuracy i.e the accuracy value with in one batch of the training data\n",
    "            correct = accuracy(inputs, labels, model)\n",
    "        \n",
    "            # Append the loss of each batch to the batches_losses variable\n",
    "            batch_accuracies.extend(correct)\n",
    "        \n",
    "        # After iterating over all batches, computed each batch accuracies values in the train loader, \n",
    "        #we Compute the average of this batch Accuracies  for every epoch and this make \"epoch_training_accuracy\" variable\n",
    "        epoch_training_accuracy = np.mean(batch_accuracies)\n",
    "    \n",
    "        #\n",
    "        training_accuracies.append(epoch_training_accuracy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        # Iterate through the batches of the test/validation data loader and compute the validation loss and accuracy\n",
    "        for i, data in enumerate(iter(test_loader)):\n",
    "            \n",
    "            # Get the test inputs and labels and since we are using all the validation as our batch size\n",
    "            inputs, labels = data\n",
    "        \n",
    "            # Compute the accuracy of the prediction on the validation set\n",
    "            val_correct = accuracy(inputs, labels, model)\n",
    "        \n",
    "        \n",
    "            # Compute the loss of the prediction on the validation set\n",
    "            val_loss = validation_loss(inputs, labels, model, loss_fn)\n",
    "            \n",
    "        # After iterating over all batches, computed each batch accuracies values in the train loader, \n",
    "        #we Compute the average of this batch Accuracies  for every epoch and this make \"epoch_training_accuracy\" variable\n",
    "        epoch_validation_accuracy = np.mean(val_correct)\n",
    "        validation_accuracies.append(epoch_validation_accuracy)\n",
    "    \n",
    "        #\n",
    "        validation_losses.append(val_loss)\n",
    "    \n",
    "    return training_losses, training_accuracies, validation_losses, validation_accuracies\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0747cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accuracies,val_losses, val_accuracies=train_model(3, model, loss_fn, optimizer,train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75d6baec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9485833333333333, 0.94555, 0.9368833333333333]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "148e83e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9453, 0.937, 0.931]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
